{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# plt.rcParams['font.family'] = 'serif'\n",
    "# plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "# plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.labelweight'] = 'normal'\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 8\n",
    "#plt.rcParams['figure.titlesize'] = 15\n",
    "#plt.rcParams['figure.titleweight'] = 'bold'\n",
    "\n",
    "#plt.rc('lines', linewidth=4)\n",
    "#plt.rc('axes', prop_cycle=(cycler(color=['r', 'g', 'b', 'y']) +\n",
    "#                           cycler(linestyle=['-', '--', ':', '-.'])))\n",
    "plt.rc('axes', prop_cycle=(cycler(color=['red', 'green', 'blue', 'yellow','cyan', 'magenta',\\\n",
    "                                         'black', 'orange', 'maroon', 'lime', 'aqua', \\\n",
    "                                         'indigo', 'darkviolet', 'dimgray', 'deeppink'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2(ax, data, field, plot_term, params, smooth = None):\n",
    "    '''data is already in filtered form\n",
    "    data: {name:list of data_points} '''\n",
    "    def last(a):\n",
    "        return a[-1]\n",
    "\n",
    "    colours = ['red', 'green', 'blue', 'yellow','cyan', 'magenta',\\\n",
    "                'black', 'orange', 'maroon', 'lime', 'aqua', \\\n",
    "                'indigo', 'darkviolet', 'dimgray', 'deeppink']\n",
    "\n",
    "    all_plots = []\n",
    "   # all_points = {}\n",
    "    lookup = set()\n",
    "    exclude_last_points = 1\n",
    "    this_cmap = {}\n",
    "    c_cpy = colours[:]\n",
    "    c_taken = params[\"cmap\"].values()\n",
    "    for c in c_taken:\n",
    "        if c in c_cpy:\n",
    "            c_cpy.remove(c)\n",
    "    for i,(k, v) in enumerate(data.items()):\n",
    "        if plot_term in k:\n",
    "            exp_id = k.split(\"_\")\n",
    "            ind = exp_id.index(plot_term)\n",
    "            plot_term_value = exp_id[ind+1]\n",
    "            if not plot_term_value in lookup:\n",
    "                lookup.add(plot_term_value)\n",
    "                label = params[\"aliasplotterm\"] + ' = ' + plot_term_value\n",
    "                if not smooth is None:\n",
    "                    new_data = moving_average_filter(v[field], smooth)\n",
    "                    exclude_last_points = smooth + 1\n",
    "                else:\n",
    "                    new_data = v[field][:]\n",
    "                x = np.arange(len(new_data))\n",
    "                \n",
    "                if label in params[\"cmap\"]:\n",
    "                    c_hldr = params[\"cmap\"][label]\n",
    "                else:\n",
    "                    c_hldr = c_cpy[0]\n",
    "                    del c_cpy[0]\n",
    "                this_cmap[label] = c_hldr\n",
    "                #all_points[label] = (x, new_data, float(plot_term_value))\n",
    "                handle = ax.plot(x[7:-exclude_last_points], new_data[7:-exclude_last_points], label = label, color = c_hldr)[0]\n",
    "                all_plots.append((handle,label, float(plot_term_value)))\n",
    "\n",
    "\n",
    "  #  all_points = {k: v for k, v in sorted(all_points.items(), key=lambda item: item[1][2], reverse = True)}\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    label_id = [float(l.split(\" = \")[-1]) for l in labels]\n",
    "    all_plots = [tuple(i) for i in zip(handles, labels, label_id)]\n",
    "   # all_plots.sort(key = last, reverse=True)\n",
    "\n",
    "\n",
    "   # print(handles, labels)\n",
    "\n",
    "    ax.set_xlabel(params[\"xlabel\"], fontsize = params[\"xlabelsize\"])  # Add an x-label to the axes.\n",
    "    ax.set_ylabel(params[\"ylabel\"], fontsize = params[\"ylabelsize\"])  # Add a y-label to the axes.\n",
    "    ax.set_title(params[\"title\"], fontsize = params[\"titlesize\"])  # Add a title to the axes.\n",
    "\n",
    "  #  handles = [p[0] for p in all_plots]\n",
    "   # lbls = [p[1] for p in all_plots]\n",
    "    h,l,_ = zip(*all_plots)\n",
    "    return h, l, this_cmap\n",
    "    #ax.legend(h, l)  # Add a legend.\n",
    "\n",
    "    \n",
    "#############################################################################################################\n",
    "\n",
    "def plot4(ax, data,field, plts, params, smooth = None):\n",
    "    ''' Plots the key-label pairs in plt where the key is a data item (single plot)'''\n",
    "    colours = ['red', 'green', 'blue', 'yellow','cyan', 'magenta',\\\n",
    "                'black', 'orange', 'maroon', 'lime', 'aqua', \\\n",
    "                'indigo', 'darkviolet', 'dimgray', 'deeppink']\n",
    "   # all_plots = []\n",
    "    \n",
    "    this_cmap = {}\n",
    "    c_cpy = colours[:]\n",
    "    c_taken = params[\"cmap\"].values()\n",
    "    for c in c_taken:\n",
    "        if c in c_cpy:\n",
    "            c_cpy.remove(c)\n",
    "    \n",
    "    for plt_key, plt_label in plts.items():\n",
    "        assert plt_key in data\n",
    "        #Data:\n",
    "        d = data[plt_key]\n",
    "        if not smooth is None:\n",
    "            smooth_data = moving_average_filter(d[field], smooth)\n",
    "            exclude_last_points = smooth + 1\n",
    "        else:\n",
    "            smooth_data = d[field]\n",
    "            exclude_last_points = 1\n",
    "        x = np.arange(len(smooth_data))\n",
    "        #Colours:\n",
    "        if plt_label in params[\"cmap\"]:\n",
    "            c_hldr = params[\"cmap\"][plt_label]\n",
    "        else:\n",
    "            c_hldr = c_cpy[0]\n",
    "            del c_cpy[0]\n",
    "        this_cmap[plt_label] = c_hldr\n",
    "        #Plot:\n",
    "        handle = ax.plot(x[7:-exclude_last_points], smooth_data[7:-exclude_last_points], label = plt_label, color = c_hldr)[0]\n",
    "      #  all_plots.append((handle,label, float(plot_term_value)))\n",
    "        \n",
    "        \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    ax.set_xlabel(params[\"xlabel\"], fontsize = params[\"xlabelsize\"])  # Add an x-label to the axes.\n",
    "    ax.set_ylabel(params[\"ylabel\"], fontsize = params[\"ylabelsize\"])  # Add a y-label to the axes.\n",
    "    ax.set_title(params[\"title\"], fontsize = params[\"titlesize\"])  # Add a title to the axes.\n",
    "    \n",
    "    return handles, labels, this_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "def get_mean_std(data, last_data, field, keys_alias_dict = None, print_ = True):\n",
    "    '''Returns a table of mean std of data.\n",
    "    keys_alias_dict can specify the keys in data to be\n",
    "    plotted. The value item can specify an alias term, or\n",
    "    be none to use the key as data label.'''\n",
    "    csv_records = []\n",
    "    #sep = \",\"\n",
    "    #end_char = \"\\n\"\n",
    "    table = []\n",
    "    hdrs = [\"Item\", \"Mean\", \"Std\"]\n",
    "    if keys_alias_dict is None:\n",
    "        for k,v in data.items():\n",
    "            new_d = np.array(v[field][-last_data:])\n",
    "            m = new_d.mean()\n",
    "            std = new_d.std()\n",
    "            hldr = [k,m,std]\n",
    "            table.append(hldr)\n",
    "            csv_records.append(hldr)\n",
    "    else:\n",
    "        for k, ak in keys_alias_dict.items():\n",
    "            new_d = np.array(data[k][field][-last_data:])\n",
    "            m = new_d.mean()\n",
    "            std = new_d.std()\n",
    "            if ak is None:\n",
    "                ak = k\n",
    "            hldr = [ak,m,std]\n",
    "            table.append(hldr)\n",
    "            csv_records.append(hldr)\n",
    "\n",
    "    if print_:\n",
    "        print(tabulate(table, headers = hdrs))\n",
    "    return csv_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(table_dict, name):\n",
    "    ''' table_dict contains {header: [row_label, mean, std]}'''\n",
    "    sep = \",\"\n",
    "    line_end = \"\\n\"\n",
    "    rows = {}\n",
    "    header = \"Architecture\"\n",
    "    for k,v in table_dict.items():\n",
    "        header += sep + k\n",
    "        for r in v:\n",
    "            m = str(np.round(r[1], 2))\n",
    "            std = str(np.round(r[2],1))\n",
    "            if not r[0] in rows:\n",
    "                rows[r[0]] = r[0]\n",
    "            rows[r[0]] += sep + m + \"$\\pm$\" + std\n",
    "    csv_txt = header + line_end\n",
    "    for v in rows.values():\n",
    "        csv_txt += v + line_end\n",
    "    f = open(make_exist(name), 'w')\n",
    "    f.write(csv_txt)\n",
    "    f.close()\n",
    "    #print(csv_txt)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Cooperative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. With increasing reward depending on how many agents are on the goal:\n",
    "(search navigation-v6_1)\n",
    "\n",
    "rewad structure:\n",
    "\n",
    "## stepr = nagennts * -0.2\n",
    "\n",
    "object_collision = -0.05 #-0.015#-0.1  #Changed to 0.0\n",
    "        agent_collision = -0.4\n",
    "        goal_reached= 0.05\n",
    "        finish_episode = 0.0\n",
    "        \n",
    "        \n",
    "## for reaching goals agents receive reward: (n_agents_on_goal**2.5) * rewards.goal_reached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment the learning curves and rendering show that MAAC converges to a suboptimal equilibrium whilst PPO does not. MAAC agents only allows three agents to finish whilst the last agent waits, even though it is resulting in less reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/home/james/Desktop/Gridworld/EXPERIMENTS/2A2'\n",
    "all_data = get_event_data(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['benchmark_2999', 'benchmark_9999', 'benchmark_5999', 'benchmark_8999', 'benchmark_99999', 'benchmark_15000', 'benchmark_29999', 'benchmark_1499', 'benchmark_149999', 'benchmark_75000', 'benchmark_60000', 'benchmark_30000', 'benchmark_90000', 'benchmark_45000'])\n"
     ]
    }
   ],
   "source": [
    "print(all_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/james/anaconda3/envs/gridworld/lib/python3.7/site-packages/tensorflow/python/summary/summary_iterator.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    }
   ],
   "source": [
    "#navigation-v6_1\n",
    "#Benchmark plots\n",
    "\n",
    "ppo_source = \"/home/james/Desktop/Gridworld/EXPERIMENTS/2A2/2A2_ppo_globalr_arc_primal6_env_independent_navigation-v6_1_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1_N0/benchmark\"\n",
    "ppo_data = get_event_data(ppo_source)\n",
    "maac_source = \"/home/james/Desktop/Gridworld/EXPERIMENTS/2A2/2A2_maac_globalr__arc_primal6_env_independent_navigation-v6_1_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1_0/benchmark\"\n",
    "maac_data = get_event_data(maac_source)\n",
    "\n",
    "combine_data = {}\n",
    "\n",
    "combine_data[\"ppo\"] = ppo_data[\"benchmark_9999\"]\n",
    "combine_data[\"maac\"] = maac_data[\"benchmark_99999\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item       Mean      Std\n",
      "------  -------  -------\n",
      "ppo     18.8632  9.15781\n",
      "maac    24.8421  4.91467\n",
      "Item       Mean       Std\n",
      "------  -------  --------\n",
      "ppo     1.74737   6.08876\n",
      "maac    3.09474  11.5995\n",
      "Item       Mean     Std\n",
      "------  -------  ------\n",
      "ppo     1.68085  3.0743\n",
      "maac    1.52128  5.3688\n",
      "Item        Mean       Std\n",
      "------  --------  --------\n",
      "ppo     0.771277  0.246409\n",
      "maac    0.704787  0.150189\n",
      "Item         Mean       Std\n",
      "------  ---------  --------\n",
      "ppo     0.414894   0.492704\n",
      "maac    0.0531915  0.224415\n"
     ]
    }
   ],
   "source": [
    "table_data = {}\n",
    "\n",
    "field = \"total_steps\"\n",
    "table_data[field] = get_mean_std(combine_data, -100, field)\n",
    "\n",
    "field = \"total_agent_collisions\"\n",
    "table_data[field] = get_mean_std(combine_data, -100, field)\n",
    "\n",
    "\n",
    "field = \"total_obstacle_collisions\"\n",
    "table_data[field] = get_mean_std(combine_data, -100, field)\n",
    "\n",
    "field = \"agent_dones\"\n",
    "table_data[field] = get_mean_std(combine_data, -100, field)\n",
    "\n",
    "field = \"all_agents_on_goal\"\n",
    "table_data[field] = get_mean_std(combine_data, -100, field)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all_agents_on_goal': [['ppo', 0.4148936170212766, 0.49270366710252816], ['maac', 0.05319148936170213, 0.22441513946520195]], 'agent_dones': [['ppo', 0.7712765957446809, 0.24640925152979723], ['maac', 0.7047872340425532, 0.15018944578186705]], 'total_agent_collisions': [['ppo', 1.7473684210526317, 6.088761728839594], ['maac', 3.094736842105263, 11.59952238963269]], 'total_obstacle_collisions': [['ppo', 1.6808510638297873, 3.0743024324069226], ['maac', 1.5212765957446808, 5.368800181751962]]}\n"
     ]
    }
   ],
   "source": [
    "print(table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = \"tables/2/global_v6_1.csv\"\n",
    "make_csv(table_data, where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single sparse reward when all agents reach the goal. Agents not penalized for collisions\n",
    "(search navigation-v5_1)\n",
    "\n",
    "Both policies perform very poorly, although PPO performs better in that it shows a continual improvement whilst MAAC shows some signs of learning and then converges to random a policy where no agents reach their goals. This is unexpected since MAAC is off policy and especially suited for sparse global rewards, making use of counterfactual rewards. \n",
    "\n",
    "Agents do not learn to avoid collisions since no penalies are given, although avoiding collisions whould mean reaching the goal in fewer time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/home/james/Desktop/Gridworld/CENTRAL_TENSORBOARD/2A2\"\n",
    "all_data = get_event_data(source, False)\n",
    "\n",
    "alias_plts = {}\n",
    "alias_plts[\"2A2_maac_globalr__arc_primal6_env_independent_navigation-v5_1_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1_N2\"] = \"MAAC\"\n",
    "alias_plts[\"2A2_ppo_globalr_arc_primal6_env_independent_navigation-v5_1_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"PPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['benchmark_15000', 'benchmark_99999', 'benchmark_75000', 'benchmark_60000', 'benchmark_30000', 'benchmark_90000', 'benchmark_45000'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppo_source = \"/home/james/Desktop/Gridworld/EXPERIMENTS/2A2/2A2_ppo_globalr_arc_primal6_env_independent_navigation-v6_1_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1_N0/benchmark\"\n",
    "ppo_source = \"/home/james/Desktop/Gridworld/EXPERIMENTS/2A2/2A2_ppo_globalr_arc_primal6_env_independent_navigation-v5_1_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1_N0\"\n",
    "ppo_data = get_event_data(ppo_source)\n",
    "#maac_source = \"/home/james/Desktop/Gridworld/EXPERIMENTS/2A2/2A2_maac_globalr__arc_primal6_env_independent_navigation-v6_1_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1_0/benchmark\"\n",
    "\n",
    "maac_data = get_event_data(maac_source)\n",
    "\n",
    "combine_data = {}\n",
    "\n",
    "combine_data[\"ppo\"] = ppo_data[\"benchmark_9999\"]\n",
    "combine_data[\"maac\"] = maac_data[\"benchmark_99999\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item           Mean         Std\n",
      "------  -----------  ----------\n",
      "MAAC    2.00602e-05  0.00447881\n",
      "PPO     0.00161059   0.0123414\n",
      "Item          Mean        Std\n",
      "------  ----------  ---------\n",
      "MAAC    0.00584253  0.0405852\n",
      "PPO     0.0842353   0.0530973\n",
      "Item       Mean      Std\n",
      "------  -------  -------\n",
      "MAAC    11.1458  6.32042\n",
      "PPO     10.7072  1.89848\n",
      "Item       Mean      Std\n",
      "------  -------  -------\n",
      "MAAC    12.7357  5.05053\n",
      "PPO     11.9276  1.49884\n"
     ]
    }
   ],
   "source": [
    "table_data = {}\n",
    "field = \"all_agents_on_goal\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "\n",
    "field = \"agent_dones\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "\n",
    "field = \"total_agent_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"total_obstacle_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = \"tables/2/global_v5_1.csv\"\n",
    "make_csv(table_data, where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment redo   (2A2t_Central):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/home/james/Desktop/Gridworld/CENTRAL_TENSORBOARD/2A2t_Central\"\n",
    "all_data = get_event_data(source, False)\n",
    "\n",
    "alias_plts = {}\n",
    "alias_plts[\"2A2_ppo_globalr_arc_primal6_env_independent_navigation-v5_1_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"PPO\"\n",
    "alias_plts[\"2A2_maac_globalr__arc_primal6_env_independent_navigation-v5_1_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1_N1\"] = \"MAAC\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item       Mean       Std\n",
      "------  -------  --------\n",
      "PPO     25.9484  0.270007\n",
      "MAAC    25.9928  0.353285\n",
      "Item       Mean      Std\n",
      "------  -------  -------\n",
      "PPO     10.618   1.89072\n",
      "MAAC    10.7513  5.94715\n",
      "Item       Mean      Std\n",
      "------  -------  -------\n",
      "PPO     11.8402  1.52845\n",
      "MAAC    11.9918  4.68924\n",
      "Item         Mean        Std\n",
      "------  ---------  ---------\n",
      "PPO     0.108184   0.0749378\n",
      "MAAC    0.0617811  0.122094\n",
      "Item          Mean        Std\n",
      "------  ----------  ---------\n",
      "PPO     0.00512412  0.0237315\n",
      "MAAC    0.0005606   0.0236704\n"
     ]
    }
   ],
   "source": [
    "table_data = {}\n",
    "\n",
    "field = \"total_steps\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"total_agent_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"total_obstacle_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"agent_dones\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"all_agents_on_goal\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = \"tables/2/global_v5_1.csv\"\n",
    "make_csv(table_data, where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agens receive a single sparse reward, but are penalized (global penalty) for collisions.\n",
    "\n",
    "Similarly to nr. 2, PPO outperforms MAAC. Both polices learn to avoid collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_source = \"/home/james/Desktop/Gridworld/EXPERIMENTS/2A2/2A2_ppo_globalr_arc_primal6_env_independent_navigation-v5_2_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1_N0/benchmark\"\n",
    "ppo_data = get_event_data(ppo_source)\n",
    "maac_source = \"/home/james/Desktop/Gridworld/EXPERIMENTS/2A2/2A2_maac_globalr__arc_primal6_env_independent_navigation-v5_2_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1_0/benchmark\"\n",
    "maac_data = get_event_data(maac_source)\n",
    "\n",
    "combine_data = {}\n",
    "\n",
    "combine_data[\"ppo\"] = ppo_data[\"benchmark_9999\"]\n",
    "combine_data[\"maac\"] = maac_data[\"benchmark_99999\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item         Mean       Std\n",
      "------  ---------  --------\n",
      "ppo     0.0957447  0.294241\n",
      "maac    0          0\n",
      "Item          Mean       Std\n",
      "------  ----------  --------\n",
      "ppo     0.295213    0.298406\n",
      "maac    0.00265957  0.025648\n",
      "Item        Mean      Std\n",
      "------  --------  -------\n",
      "ppo     0.526316  1.20387\n",
      "maac    0         0\n",
      "Item       Mean      Std\n",
      "------  -------  -------\n",
      "ppo     1.20213  1.30123\n",
      "maac    0        0\n"
     ]
    }
   ],
   "source": [
    "table_data = {}\n",
    "field = \"all_agents_on_goal\"\n",
    "table_data[field] = get_mean_std(combine_data, -100, field)\n",
    "\n",
    "\n",
    "field = \"agent_dones\"\n",
    "table_data[field] = get_mean_std(combine_data, -100, field)\n",
    "\n",
    "\n",
    "field = \"total_agent_collisions\"\n",
    "table_data[field] = get_mean_std(combine_data, -100, field)\n",
    "\n",
    "field = \"total_obstacle_collisions\"\n",
    "table_data[field] = get_mean_std(combine_data, -100, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = \"tables/2/global_v5_2.csv\"\n",
    "make_csv(table_data, where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment redo   (2A2t_Central):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/home/james/Desktop/Gridworld/CENTRAL_TENSORBOARD/2A2t_Central\"\n",
    "all_data = get_event_data(source, False)\n",
    "\n",
    "alias_plts = {}\n",
    "alias_plts[\"2A2_ppo_globalr_arc_primal6_env_independent_navigation-v5_2_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"PPO\"\n",
    "alias_plts[\"2A2_maac_globalr__arc_primal6_env_independent_navigation-v5_2_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1_N4\"] = \"MAAC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item       Mean        Std\n",
      "------  -------  ---------\n",
      "PPO     23.7748  2.87186\n",
      "MAAC    25.9998  0.0401451\n",
      "Item        Mean      Std\n",
      "------  --------  -------\n",
      "PPO     1.14971   1.16557\n",
      "MAAC    0.304464  1.25581\n",
      "Item       Mean      Std\n",
      "------  -------  -------\n",
      "PPO     2.83043  2.40414\n",
      "MAAC    1.50558  2.05824\n",
      "Item         Mean        Std\n",
      "------  ---------  ---------\n",
      "PPO     0.415641   0.252292\n",
      "MAAC    0.0241927  0.0797579\n",
      "Item           Mean         Std\n",
      "------  -----------  ----------\n",
      "PPO     0.158096     0.185855\n",
      "MAAC    4.00416e-05  0.00632772\n"
     ]
    }
   ],
   "source": [
    "table_data = {}\n",
    "\n",
    "field = \"total_steps\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"total_agent_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"total_obstacle_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"agent_dones\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"all_agents_on_goal\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = \"tables/2/global_v5_2.csv\"\n",
    "make_csv(table_data, where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step reward varied:\n",
    "\n",
    "sr_-0.01_ocr_-0.015_acr_-0.4_grr_0.1_fer_0.0\n",
    "\n",
    "with stepr = -0.01   -0.1  -0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/home/james/Desktop/Gridworld/CENTRAL_TENSORBOARD/2A2\"\n",
    "all_data = get_event_data(source, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source = \"/home/james/Desktop/Gridworld/CENTRAL_TENSORBOARD/2A2\"\n",
    "#all_data = get_event_data(source, False)\n",
    "\n",
    "alias_plts = {}\n",
    "alias_plts[\"2A2_ppo_arc_primal6_sr_-0.01_ocr_-0.015_acr_-0.4_grr_0.1_fer_0.0_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"PPO_-0.01\"\n",
    "alias_plts[\"2A2_ppo_arc_primal6_sr_-0.1_ocr_-0.015_acr_-0.4_grr_0.1_fer_0.0_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"PPO_-0.1\"\n",
    "alias_plts[\"2A2_ppo_arc_primal6_sr_-0.4_ocr_-0.015_acr_-0.4_grr_0.1_fer_0.0_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"PPO_-0.4\"\n",
    "\n",
    "\n",
    "alias_plts[\"2A2_maac__arc_primal6_sr_-0.01_ocr_-0.015_acr_-0.4_grr_0.1_fer_0.0_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"MAAC_-0.01\"\n",
    "alias_plts[\"2A2_maac__arc_primal6_sr_-0.1_ocr_-0.015_acr_-0.4_grr_0.1_fer_0.0_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"MAAC_-0.1\"\n",
    "alias_plts[\"2A2_maac__arc_primal6_sr_-0.4_ocr_-0.015_acr_-0.4_grr_0.1_fer_0.0_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"MAAC_-0.4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item           Mean       Std\n",
      "----------  -------  --------\n",
      "PPO_-0.01   29.2713   4.44429\n",
      "PPO_-0.1    19.0827   2.6807\n",
      "PPO_-0.4    17.9478   2.93486\n",
      "MAAC_-0.01  29.3837  13.3352\n",
      "MAAC_-0.1   21.4935   7.96681\n",
      "MAAC_-0.4   20.6079   8.52833\n",
      "Item            Mean      Std\n",
      "----------  --------  -------\n",
      "PPO_-0.01   2.1733    1.72579\n",
      "PPO_-0.1    1.64165   1.26335\n",
      "PPO_-0.4    2.35789   1.80451\n",
      "MAAC_-0.01  0.490504  2.43312\n",
      "MAAC_-0.1   0.669164  2.40663\n",
      "MAAC_-0.4   1.53288   4.01316\n",
      "Item           Mean      Std\n",
      "----------  -------  -------\n",
      "PPO_-0.01   6.87067  2.65474\n",
      "PPO_-0.1    4.51371  1.69321\n",
      "PPO_-0.4    3.56879  1.74252\n",
      "MAAC_-0.01  3.64182  4.80418\n",
      "MAAC_-0.1   3.48334  3.98492\n",
      "MAAC_-0.4   4.23595  4.45753\n",
      "Item            Mean        Std\n",
      "----------  --------  ---------\n",
      "PPO_-0.01   0.755193  0.0884213\n",
      "PPO_-0.1    0.798107  0.0682543\n",
      "PPO_-0.4    0.822956  0.0686914\n",
      "MAAC_-0.01  0.675335  0.317899\n",
      "MAAC_-0.1   0.660995  0.301879\n",
      "MAAC_-0.4   0.729607  0.25061\n",
      "Item            Mean       Std\n",
      "----------  --------  --------\n",
      "PPO_-0.01   0.373031  0.16739\n",
      "PPO_-0.1    0.430046  0.147354\n",
      "PPO_-0.4    0.471578  0.153301\n",
      "MAAC_-0.01  0.332141  0.470981\n",
      "MAAC_-0.1   0.28214   0.450041\n",
      "MAAC_-0.4   0.330647  0.470446\n"
     ]
    }
   ],
   "source": [
    "table_data = {}\n",
    "\n",
    "field = \"total_steps\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"total_agent_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"total_obstacle_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"agent_dones\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "\n",
    "field = \"all_agents_on_goal\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = \"tables/2/indiv_step.csv\"\n",
    "make_csv(table_data, where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object collision varied from -0.015 to -0.4\n",
    "sr_-0.1_ocr_-0.4_acr_-0.4_grr_0.1_fer_0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_plts = {}\n",
    "\n",
    "alias_plts[\"2A2_ppo_arc_primal6_sr_-0.1_ocr_-0.015_acr_-0.4_grr_0.1_fer_0.0_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"PPO_-0.015\"\n",
    "alias_plts[\"2A2_ppo_arc_primal6_sr_-0.1_ocr_-0.4_acr_-0.4_grr_0.1_fer_0.0_disc_0.5_lambda_1.0_entropy_0.01_minibatch_512_rollouts_256_workers_4_kepochs_8_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"PPO_-0.4\"\n",
    "\n",
    "alias_plts[\"2A2_maac__arc_primal6_sr_-0.1_ocr_-0.015_acr_-0.4_grr_0.1_fer_0.0_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"MAAC_-0.015\"\n",
    "alias_plts[\"2A2_maac__arc_primal6_sr_-0.1_ocr_-0.4_acr_-0.4_grr_0.1_fer_0.0_disc_0.9_rewardscale_10_minibatch_1024_nupdates_100_attheads_4_envsize_5_nagents_4_objdensity_0.2_seed_1\"] = \"MAAC_-0.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item            Mean      Std\n",
      "-----------  -------  -------\n",
      "PPO_-0.015   19.0827  2.6807\n",
      "PPO_-0.4     18.5902  2.95923\n",
      "MAAC_-0.015  21.4935  7.96681\n",
      "MAAC_-0.4    26       0\n",
      "Item              Mean      Std\n",
      "-----------  ---------  -------\n",
      "PPO_-0.015   1.64165    1.26335\n",
      "PPO_-0.4     1.36809    1.44436\n",
      "MAAC_-0.015  0.669164   2.40663\n",
      "MAAC_-0.4    0.0171918  0.19104\n",
      "Item              Mean       Std\n",
      "-----------  ---------  --------\n",
      "PPO_-0.015   4.51371    1.69321\n",
      "PPO_-0.4     0.564455   1.00194\n",
      "MAAC_-0.015  3.48334    3.98492\n",
      "MAAC_-0.4    0.0145829  0.128755\n",
      "Item               Mean        Std\n",
      "-----------  ----------  ---------\n",
      "PPO_-0.015   0.798107    0.0682543\n",
      "PPO_-0.4     0.805341    0.085025\n",
      "MAAC_-0.015  0.660995    0.301879\n",
      "MAAC_-0.4    0.00134624  0.0186358\n",
      "Item             Mean       Std\n",
      "-----------  --------  --------\n",
      "PPO_-0.015   0.430046  0.147354\n",
      "PPO_-0.4     0.445527  0.160467\n",
      "MAAC_-0.015  0.28214   0.450041\n",
      "MAAC_-0.4    0         0\n"
     ]
    }
   ],
   "source": [
    "table_data = {}\n",
    "\n",
    "field = \"total_steps\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"total_agent_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"total_obstacle_collisions\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "field = \"agent_dones\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n",
    "\n",
    "field = \"all_agents_on_goal\"\n",
    "table_data[field] = get_mean_std(all_data, -100, field, alias_plts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = \"tables/2/indiv_oc.csv\"\n",
    "make_csv(table_data, where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Collision\n",
    "\n",
    "### NB should have kept oc at the default parameter ... Need to run this experiment again in order to determine effect of agent collisions alone on performace\n",
    "\n",
    "\n",
    "#Note: Increase agent object collision penalty increases performance slightly ... it is not significant\n",
    " #But too large penalty for object collisions negatively affects MAAC. (is this because MAAC does not handle individualized rewards well due its shared critic... But performs better again when goal reached reward is increased\n",
    "\n",
    "# For MAAC best reward structures are either: stepr=0.1 ; ocr = -0.4 acr =-0.4 ; grr=1.0 or stepr=0.1 ; ocr = -0.015 acr =-0.4 ; grr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('gridworld': conda)",
   "language": "python",
   "name": "python37564bitgridworldcondaa470b01e75544e91a588ed5e9d97392f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
