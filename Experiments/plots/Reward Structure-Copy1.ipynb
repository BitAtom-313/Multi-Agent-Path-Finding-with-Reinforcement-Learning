{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Cooperative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. With increasing reward depending on how many agents are on the goal:\n",
    "(search navigation-v6_1)\n",
    "\n",
    "rewad structure:\n",
    "\n",
    "## stepr = nagennts * -0.2\n",
    "\n",
    "object_collision = -0.05 #-0.015#-0.1  #Changed to 0.0\n",
    "        agent_collision = -0.4\n",
    "        goal_reached= 0.05\n",
    "        finish_episode = 0.0\n",
    "        \n",
    "        \n",
    "## for reaching goals agents receive reward: (n_agents_on_goal**2.5) * rewards.goal_reached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment the learning curves and rendering show that MAAC converges to a suboptimal equilibrium whilst PPO does not. MAAC agents only allows three agents to finish whilst the last agent waits, even though it is resulting in less reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single sparse reward when all agents reach the goal. Agents not penalized for collisions\n",
    "(search navigation-v5_1)\n",
    "\n",
    "Both policies perform very poorly, although PPO performs better in that it shows a continual improvement whilst MAAC shows some signs of learning and then converges to random a policy where no agents reach their goals. This is unexpected since MAAC is off policy and especially suited for sparse global rewards, making use of counterfactual rewards. \n",
    "\n",
    "Agents do not learn to avoid collisions since no penalies are given, although avoiding collisions whould mean reaching the goal in fewer time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agens receive a single sparse reward, but are penalized (global penalty) for collisions.\n",
    "\n",
    "Similarly to nr. 2, PPO outperforms MAAC. Both polices learn to avoid collisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('gridworld': conda)",
   "language": "python",
   "name": "python37564bitgridworldcondaa470b01e75544e91a588ed5e9d97392f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
